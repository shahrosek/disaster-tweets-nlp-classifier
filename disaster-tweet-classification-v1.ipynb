{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":4303493,"sourceType":"datasetVersion","datasetId":2535218}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install contractions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install missingno","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pyspellchecker","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom unicodedata import normalize\nimport emoji\nfrom textblob import TextBlob\nfrom nltk.corpus import stopwords\nimport wordsegment as ws\nws.load()\nimport contractions\nfrom spellchecker import SpellChecker\nfrom nltk.stem import PorterStemmer\nimport missingno as msno","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Reading the training and testing dataset","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Basic info about training data","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dropping the ids columns as they are not needed","metadata":{}},{"cell_type":"code","source":"train_df.drop('id', axis=1, inplace=True)\ntest_df.drop('id', axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Combining the training and testing dataset","metadata":{}},{"cell_type":"code","source":"df = train_df.append(test_df)\ndf","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Reseting the index from 0 to length of dataframe","metadata":{}},{"cell_type":"code","source":"df.reset_index(drop=True, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe(include='all')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['keyword'].unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Most of the keywords have %20 in them as noise","metadata":{}},{"cell_type":"markdown","source":"# Visualizing the missing values in the complete dataset","metadata":{}},{"cell_type":"code","source":"msno.matrix(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"msno.heatmap(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Location has the most missing values and looking at the heatmap, target is not affected in any way by location, so we drop it","metadata":{}},{"cell_type":"code","source":"df.drop('location', axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Removing the %20 in the keyword feature","metadata":{}},{"cell_type":"code","source":"df['keyword'] = df['keyword'].str.replace('%20', ' ')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# As the keyword column specifies the keyword in the tweet, therefore filling the missing values with 'zero'","metadata":{}},{"cell_type":"code","source":"df['keyword'] = df['keyword'].fillna('zero')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading the stop words ","metadata":{}},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing the text column having all the tweets","metadata":{}},{"cell_type":"markdown","source":"# Function for removing the hashtags and splitting the multi word hashtag by segmenting it","metadata":{}},{"cell_type":"code","source":"def extract_words(tweet):\n    hashtags = re.findall(r\"(#\\w+)\", tweet)\n    for hs in hashtags:\n        words = \" \".join(ws.segment(hs))\n        tweet = tweet.replace(hs, words)\n    return tweet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Function for removing the urls from tweets","metadata":{}},{"cell_type":"code","source":"def remove_urls(tweet):\n    return re.sub(r'http\\S+', '', tweet)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Function for removing numbers from the tweets","metadata":{}},{"cell_type":"code","source":"def remove_numbers(tweet):\n    return re.sub(r'[^\\D\\.]', '', tweet)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Function for removing user mentions from the tweets","metadata":{}},{"cell_type":"code","source":"def remove_usermentions(tweet):\n    return re.sub(r'@(\\w+)', '', tweet)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Function for removing stop words from the tweets","metadata":{}},{"cell_type":"code","source":"def remove_stop_words(df, index, column):\n    text = ''\n\n    words = df[column][index].split(' ')\n    \n    for i in range(len(words)):\n        if words[i] not in stop_words:\n            text += words[i]\n            text += ' '\n    text = text.rstrip()\n    df.at[index, column] = text\n    \n    return df[column][index]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Functions for removing punctuations","metadata":{}},{"cell_type":"code","source":"def remove_punctuations(tweet):\n    return re.sub(r'[^\\w\\s]', '', tweet)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Function for creating stems of all the words in the tweets","metadata":{}},{"cell_type":"code","source":"def stem_words(tweet):\n    ps = PorterStemmer()\n    \n    word_list = tweet.split()\n    \n    stems = ' '.join([ps.stem(w) for w in word_list])\n    \n    return stems","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# All the preprocessing is applied in this loop","metadata":{}},{"cell_type":"markdown","source":"1- First we remove the unicode characters\n2- Then we remove the user mentions\n3- Then we remove the hashtags\n4- Then remove the contractions\n5- Then we convert the emojis to their full text form\n6- Then we remove urls\n7- Then we remove numbers\n8- Then we remove punctuations\n9- Then we remove the stop words\n10- Then we lower case the words\n11- Then we remove unwanted spaces\n12- Then we create stems for all the words in tweets","metadata":{}},{"cell_type":"code","source":"for index in df.index:\n    \n    # remove unicode characters\n    df.at[index, 'text'] = (normalize('NFKD', df['text'][index]).encode('ascii','ignore')).decode('utf-8')\n    df.at[index, 'keyword'] = (normalize('NFKD', df['keyword'][index]).encode('ascii','ignore')).decode('utf-8')\n    \n    # remove user mentions\n    df.at[index, 'text'] = remove_usermentions(df['text'][index])\n    df.at[index, 'keyword'] = remove_usermentions(df['keyword'][index])\n    \n    # remove hashtags and splits the words\n    df.at[index, 'text'] = extract_words(df['text'][index])\n    df.at[index, 'keyword'] = extract_words(df['keyword'][index])\n    \n    # remove contractions\n    df.at[index, 'text'] = contractions.fix(df['text'][index])\n    df.at[index, 'keyword'] = contractions.fix(df['keyword'][index])\n    \n    # convert emojis into text\n    df.at[index, 'text'] = emoji.demojize(df['text'][index], delimiters=(\"\", \"\"))\n    df.at[index, 'keyword'] = emoji.demojize(df['keyword'][index], delimiters=(\"\", \"\"))\n   \n    # remove urls\n    df.at[index, 'text'] = remove_urls(df['text'][index])\n    df.at[index, 'keyword'] = remove_urls(df['keyword'][index])\n    \n    # remove numbers\n    df.at[index, 'text'] = remove_numbers(df['text'][index])\n    df.at[index, 'keyword'] = remove_numbers(df['keyword'][index])\n   \n    # remove punctuations\n    df.at[index, 'text'] = remove_punctuations(df['text'][index])\n    df.at[index, 'keyword'] = remove_punctuations(df['keyword'][index])\n   \n    # removing stop words\n    df.at[index, 'text'] = remove_stop_words(df, index, 'text')\n    df.at[index, 'keyword'] = remove_stop_words(df, index, 'keyword')\n    \n    # lower casing the words\n    df.at[index, 'text'] = df['text'][index].lower()\n    df.at[index, 'keyword'] = df['keyword'][index].lower()\n    \n    # removing unwanted white spaces\n    df.at[index, 'text'] = ' '.join(df['text'][index].split())\n    df.at[index, 'keyword'] = ' '.join(df['keyword'][index].split())\n   \n    # create stems\n    df.at[index, 'text'] = stem_words(df['text'][index])\n    df.at[index, 'keyword'] = stem_words(df['keyword'][index])\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# We are combining the keywords with tweets and creating a new column","metadata":{}},{"cell_type":"code","source":"df['text_with_keywords'] = df['text'] + ' ' + df['keyword']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dropping the text column as it is no longer needed","metadata":{}},{"cell_type":"code","source":"df.drop('text', inplace=True, axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dropping the keyword column as it is no longer needed","metadata":{}},{"cell_type":"code","source":"df.drop('keyword', inplace=True, axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Separating the dataset into training and testing dataframes","metadata":{}},{"cell_type":"code","source":"train = df.iloc[0:7612, ]\ntest = df.iloc[7613:10876, ].drop('target',axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"markdown","source":"# Splitting the training and testing features","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = train.drop('target', axis=1)\ny = train['target']\n\nX_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.3, random_state=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_validation","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creating word vectors","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer()\n\nX_train_bow = cv.fit_transform(X_train['text_with_keywords']).toarray()\nX_validation_bow = cv.transform(X_validation['text_with_keywords']).toarray()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_bow.shape, X_validation_bow.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_bow.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_validation_bow.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing the data on Gaussian Naive Bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\n\ngnb_model = gnb.fit(X_train_bow, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_validation = gnb_model.predict(X_validation_bow)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Checking the accuracy scores, confusion matrix and f1 score on  Gaussain NB results ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\naccuracy_score(y_validation, y_pred_validation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"confusion_matrix(y_validation, y_pred_validation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f1_score(y_validation, y_pred_validation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing the data on Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrand_forest = RandomForestClassifier()\n\nmodel_randforest = rand_forest.fit(X_train_bow, y_train)\n\ny_pred = model_randforest.predict(X_validation_bow)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Checking the accuracy scores, confusion matrix and f1 score on Random Forest Classifier results","metadata":{}},{"cell_type":"code","source":"accuracy_score(y_validation, y_pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"confusion_matrix(y_validation, y_pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f1_score(y_validation, y_pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Making Bi-Gram features and training and testing on CatBoost Classifier","metadata":{}},{"cell_type":"code","source":"cv = CountVectorizer(ngram_range = (1, 2), max_features=7000)\n\nX_train_bow_bigram = cv.fit_transform(X_train['text_with_keywords']).toarray()\nX_validation_bow_bigram = cv.transform(X_validation['text_with_keywords']).toarray()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test_bow_bigram = cv.transform(test['text_with_keywords']).toarray()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing the data on Cat Boost Classifier","metadata":{}},{"cell_type":"code","source":"import catboost as cb\n\ncat_boost = cb.CatBoostClassifier(loss_function='CrossEntropy', iterations=10000, od_wait=100, od_type='Iter')\n\nmodel_cb = cat_boost.fit(X_train_bow_bigram, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prediciting on validation data","metadata":{}},{"cell_type":"code","source":"y_predict_cb = model_cb.predict(X_validation_bow)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Predicting on test data","metadata":{}},{"cell_type":"code","source":"y_predict_cb_test = model_cb.predict(X_test_bow_bigram)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_predict_cb_test.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_predict_cb.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Checking the accuracy scores, confusion matrix and f1 score on Cat Boost Classifier results of validation data","metadata":{}},{"cell_type":"code","source":"accuracy_score(y_validation, y_predict_cb)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"confusion_matrix(y_validation, y_predict_cb)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f1_score(y_validation, y_predict_cb)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Making word vectors using Bi Gram model and testing on Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"model_randforest = rand_forest.fit(X_train_bow_bigram, y_train)\n\ny_pred = model_randforest.predict(X_validation_bow_bigram)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_validation_bow_bigram.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Checking the accuracy scores, confusion matrix and f1 score on Random Forest Classifier for Bi gram model results","metadata":{}},{"cell_type":"code","source":"accuracy_score(y_validation, y_pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f1_score(y_validation, y_pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"confusion_matrix(y_validation, y_pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_test = model_randforest.predict(X_test_bow_bigram)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_test.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submission['target'] = pd.DataFrame(y_predict_cb_test, columns=['target'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submission.to_csv('sample_submission5.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv('nlp_tweets.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}